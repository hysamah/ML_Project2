{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_neg.txt', 'train_pos.txt', 'sample_submission.csv', 'test_data.txt', 'train_neg_clean.txt', 'train_pos_clean.txt', 'test_data_clean.txt']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from course_helpers import *\n",
    "from helpers import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Module, LSTM, ReLU, Linear, Sigmoid, Dropout, Embedding\n",
    "import torchvision\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "##data preprocessing \n",
    "DATA_PATH = 'twitter-datasets'\n",
    "Dataset = read(DATA_PATH)\n",
    "\n",
    "#@title read dataset file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset = clean(Dataset)\n",
    "# save_to_file(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash course_helpers.sh\n",
    "##Run this in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cut = 'vocab_cut.txt'\n",
    "vocab_pkl = 'vocab_pkl.pkl'\n",
    "coco_pkl = 'coco_pkl.pkl'\n",
    "embd = 'embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_vocab(vocab_cut, vocab_pkl)\n",
    "# cooc_(vocab_pkl, DATA_PATH, coco_pkl)\n",
    "# glove(coco_pkl, embd)\n",
    "# with open(coco_pkl, \"rb\") as f:\n",
    "#         cooc = pickle.load(f)\n",
    "# print(cooc.shape)\n",
    "# with open(vocab_pkl, \"rb\") as f:\n",
    "#         vocab = pickle.load(f)\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17971, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding = np.load(embd+'.npy')\n",
    "print(embedding.shape)\n",
    "glove_embd = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos, train_neg, test_set = read_data(Dataset)\n",
    "# train_set = pd.concat([train_pos, train_neg],  axis=0, ignore_index=True)\n",
    "# train_set = pd.concat([train_set.drop(labels='sentiment', axis=1), train_set['sentiment']], axis = 1)\n",
    "\n",
    "# train_set.loc[:, train_set.columns!='sentiment'] = train_set.applymap(lambda x: find_token(x, vocab))\n",
    "# test_set2 =  test_set.applymap(lambda x: find_token(x, vocab))\n",
    "# train_set.to_csv('train_set_token.csv', index= False)\n",
    "# test_set2.to_csv('test_set_token.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test_set_token.csv')\n",
    "train_set = pd.read_csv('train_set_token.csv', )\n",
    "num_embeddings  = glove_embd.shape[0]\n",
    "embed_dim = glove_embd.shape[1]\n",
    "max_len = train_set.shape[1]-1\n",
    "batch_size = 16\n",
    "n_batches = train_set.shape[0]/batch_size\n",
    "\n",
    "X_train = train_set.loc[:, train_set.columns!='sentiment']\n",
    "Y_train = train_set['sentiment']\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "glove_embd = torch.tensor(glove_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>562</td>\n",
       "      <td>429</td>\n",
       "      <td>15</td>\n",
       "      <td>1077</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177</td>\n",
       "      <td>42</td>\n",
       "      <td>2692</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>1603</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>323</td>\n",
       "      <td>10153</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>722</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1525</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>811</td>\n",
       "      <td>574</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3977</td>\n",
       "      <td>15</td>\n",
       "      <td>481</td>\n",
       "      <td>1928</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>4212</td>\n",
       "      <td>188</td>\n",
       "      <td>1083</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179482</th>\n",
       "      <td>87</td>\n",
       "      <td>16</td>\n",
       "      <td>162</td>\n",
       "      <td>7</td>\n",
       "      <td>1336</td>\n",
       "      <td>1288</td>\n",
       "      <td>164</td>\n",
       "      <td>3</td>\n",
       "      <td>168</td>\n",
       "      <td>201</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179483</th>\n",
       "      <td>0</td>\n",
       "      <td>2921</td>\n",
       "      <td>2</td>\n",
       "      <td>424</td>\n",
       "      <td>15</td>\n",
       "      <td>1033</td>\n",
       "      <td>2967</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179484</th>\n",
       "      <td>5</td>\n",
       "      <td>9555</td>\n",
       "      <td>1688</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>948</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179485</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>172</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179486</th>\n",
       "      <td>57</td>\n",
       "      <td>265</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>482</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179487 rows Ã— 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2     3      4     5     6    7     8     9  ...  134  \\\n",
       "0          0     2    33     5     16    56   562  429    15  1077  ...    5   \n",
       "1        177    42  2692    14     27  1603     6    2     5    38  ...    5   \n",
       "2         25     0    40   323  10153    22    10  722     3    25  ...    5   \n",
       "3          0     0    88  1525    125   125    33   16   811   574  ...    5   \n",
       "4       3977    15   481  1928     14     5  4212  188  1083     5  ...    5   \n",
       "...      ...   ...   ...   ...    ...   ...   ...  ...   ...   ...  ...  ...   \n",
       "179482    87    16   162     7   1336  1288   164    3   168   201  ...    5   \n",
       "179483     0  2921     2   424     15  1033  2967    1     9     5  ...    5   \n",
       "179484     5  9555  1688     5      4     5   352  948     5     4  ...    5   \n",
       "179485     5    14   172    59      5     5     5    5     5     5  ...    5   \n",
       "179486    57   265    81    19    501     6    74  482     5     5  ...    5   \n",
       "\n",
       "        135  136  137  138  139  140  141  142  sentiment  \n",
       "0         5    5    5    5    5    5    5    5          1  \n",
       "1         5    5    5    5    5    5    5    5          1  \n",
       "2         5    5    5    5    5    5    5    5          1  \n",
       "3         5    5    5    5    5    5    5    5          1  \n",
       "4         5    5    5    5    5    5    5    5          1  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...        ...  \n",
       "179482    5    5    5    5    5    5    5    5          0  \n",
       "179483    5    5    5    5    5    5    5    5          0  \n",
       "179484    5    5    5    5    5    5    5    5          0  \n",
       "179485    5    5    5    5    5    5    5    5          0  \n",
       "179486    5    5    5    5    5    5    5    5          0  \n",
       "\n",
       "[179487 rows x 144 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         28\n",
       "1         21\n",
       "2         12\n",
       "3         19\n",
       "4         12\n",
       "          ..\n",
       "179482    13\n",
       "179483    32\n",
       "179484    18\n",
       "179485     5\n",
       "179486    10\n",
       "Name: num_uniq, Length: 179487, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram = X_train\n",
    "histogram['num_uniq'] = histogram.apply(pd.Series.nunique, axis=1)\n",
    "histogram = histogram['num_uniq']\n",
    "histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need not be sorted, necessarily\n",
    "a = (0, 1, 1, 1, 2, 3, 7, 7, 23)\n",
    "\n",
    "def count_elements(seq) -> dict:\n",
    "    \"\"\"Tally elements from `seq`.\"\"\"\n",
    "    hist = {}\n",
    "    for i in seq:\n",
    "        hist[i] = hist.get(i, 0) + 1\n",
    "    return hist\n",
    "counted =count_elements(histogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179487,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 144 artists>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist,bins = np.histogram(histogram,bins = [i for i in range(0, 145, 1)]) \n",
    "plt.bar(bins[:-1], hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = (X_train.shape[0]*0.2)%batch_size\n",
    "data_split  = X_train.shape[0]*0.2 - data_split\n",
    "data_split=data_split/2\n",
    "remd = -data_split + X_train.shape[0] - data_split\n",
    "addedv = remd%data_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val =  torch.tensor(pd.concat([X_train[:int(data_split)], X_train[-int(data_split):]],  axis=0, ignore_index=True).reset_index(drop=True).to_numpy()).to(device)\n",
    "Y_val =  torch.tensor(pd.concat([Y_train[:int(data_split)], Y_train[-int(data_split):]],  axis=0, ignore_index=True).reset_index(drop=True).to_numpy(dtype='float32')).to(device)\n",
    "X_train = torch.tensor(X_train[int(data_split+addedv):-int(data_split)].reset_index(drop=True).to_numpy()).to(device)\n",
    "Y_train =  torch.tensor(Y_train[int(data_split+addedv):-int(data_split)].reset_index(drop=True).to_numpy(dtype='float32')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =  torch.tensor(test_set.to_numpy()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_torch_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=[]):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.encodings[idx]\n",
    "        label = self.labels[idx]\n",
    "        return inp, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "class make_torch_testset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.encodings[idx]\n",
    "        return inp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "train_dataset = make_torch_dataset(X_train, Y_train)\n",
    "val_dataset = make_torch_dataset(X_val, Y_val)\n",
    "test_dataset = make_torch_testset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 143552 instances\n",
      "Validation set has 35888 instances\n",
      "Test set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(train_dataset)))\n",
    "print('Validation set has {} instances'.format(len(val_dataset)))\n",
    "print('Test set has {} instances'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class lstm_classifier(Module):\n",
    "    def __init__(self, num_embeddings, embed_dim, glove_embd, max_len) -> None:\n",
    "        super().__init__()\n",
    "        self.emd =  Embedding(num_embeddings= num_embeddings, embedding_dim= embed_dim, _weight=glove_embd)\n",
    "        self.lstm1 = LSTM(input_size = embed_dim, hidden_size = 256, bidirectional = True, batch_first  = True)\n",
    "        self.lstm2 = LSTM(input_size = 512, hidden_size = 150, bidirectional = True,  batch_first  = True)\n",
    "        self.fc1 = Linear(300,128)\n",
    "        self.drp = Dropout(0.1)\n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.fc3 = Linear(64,1)\n",
    "        self.reg = Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emd(x)\n",
    "        x = x.to(torch.float32)\n",
    "        x, states = self.lstm1(x)\n",
    "        x, states = self.lstm2(x)\n",
    "        x = F.relu(self.fc1(x[:,0]))\n",
    "        x = self.drp(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.reg(x)\n",
    "        x = x.reshape(x.shape[0])\n",
    "\n",
    "        return x\n",
    "model = lstm_classifier(num_embeddings, embed_dim, glove_embd, max_len).to(device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "\n",
    "    with tqdm(training_loader, unit=\"batch\") as tepoch:\n",
    "        for i, data in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch_index}\")\n",
    "\n",
    "\n",
    "    #for i, data in enumerate(tqdm(training_loader), unit= \"batches\", ):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            accuracy  = accuracy_score(labels.cpu(), [1 if x > 0.5 else 0 for x in outputs.cpu()])\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += accuracy\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy = accuracy)\n",
    "            if i % batch_size == batch_size-1:\n",
    "                last_loss = running_loss / batch_size # loss per batch\n",
    "                #print('  epoch {} batch {} loss: {}'.format(epoch_index, (i + 1)/batch_size, last_loss))\n",
    "                tb_x = epoch_index * len(training_loader) + i + 1\n",
    "                #print(tb_x)\n",
    "                tb_writer.add_scalar('Loss/train', last_loss, global_step = tb_x)\n",
    "                running_loss = 0.\n",
    "\n",
    "    return last_loss, running_accuracy/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/8972 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/8972 [00:00<1:51:06,  1.35batch/s, accuracy=0.562, loss=0.691]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n",
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/8972 [00:01<2:07:56,  1.17batch/s, accuracy=0.375, loss=0.702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 4/8972 [00:02<1:20:34,  1.86batch/s, accuracy=0.375, loss=0.698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "0.375\n",
      "1.875\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 4/8972 [00:02<1:20:34,  1.86batch/s, accuracy=0.25, loss=0.701] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.125\n",
      "0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 6/8972 [00:03<59:52,  2.50batch/s, accuracy=0.438, loss=0.694]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5625\n",
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 7/8972 [00:04<1:22:05,  1.82batch/s, accuracy=0.375, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9375\n",
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 8/8972 [00:05<1:48:37,  1.38batch/s, accuracy=0.375, loss=0.694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3125\n",
      "0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 9/8972 [00:06<1:53:59,  1.31batch/s, accuracy=0.438, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 10/8972 [00:07<2:01:17,  1.23batch/s, accuracy=0.5, loss=0.693] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25\n",
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 11/8972 [00:07<2:06:36,  1.18batch/s, accuracy=0.562, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8125\n",
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 11/8972 [00:08<1:56:25,  1.28batch/s, accuracy=0.562, loss=0.693]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m avg_loss, acc \u001b[39m=\u001b[39m train_one_epoch(epoch, writer)\n\u001b[1;32m     19\u001b[0m \u001b[39m# # We don't need gradients on to do reporting\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [17], line 29\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m---> 29\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     31\u001b[0m \u001b[39m# Adjust learning weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, acc = train_one_epoch(epoch, writer)\n",
    "\n",
    "    # # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs).detach()\n",
    "        vacc  = accuracy_score(vlabels.cpu(), [1 if x > 0.5 else 0 for x in voutputs.cpu()])\n",
    "        running_vacc += vacc\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vacc = running_vacc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy',\n",
    "                    { 'Training' : acc, 'Validation' : avg_vacc },\n",
    "                    epoch + 1)\n",
    "    #writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "preds_test = []\n",
    "for i, vdata in enumerate(test_loader):\n",
    "    vinputs = vdata\n",
    "    voutputs = model(vinputs).detach()\n",
    "    preds_test.extend(voutputs.cpu())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10000.0\n"
     ]
    }
   ],
   "source": [
    "preds = np.ones_like(preds_test)\n",
    "preds[np.where(np.array(preds_test)<0.5)] = -1\n",
    "preds = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1)+1, \"Prediction\": int(r2)})\n",
    "\n",
    "create_csv_submission(preds.index.values.tolist(), np.asarray(preds), \"____test____.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['train_neg.txt', 'train_pos.txt', 'sample_submission.csv', 'test_data.txt', 'train_neg_clean.txt', 'train_pos_clean.txt', 'test_data_clean.txt']\n",
      "(17971, 17971)\n",
      "17971\n",
      "(17971, 20)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from course_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "\n",
    "\n",
    "##data preprocessing \n",
    "DATA_PATH = 'twitter-datasets'\n",
    "Dataset = read(DATA_PATH)\n",
    "\n",
    "#@title read dataset file\n",
    "vocab_cut = 'vocab_cut.txt'\n",
    "vocab_pkl = 'vocab_pkl.pkl'\n",
    "coco_pkl = 'coco_pkl.pkl'\n",
    "embd = 'embeddings'\n",
    "with open(coco_pkl, \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "print(cooc.shape)\n",
    "with open(vocab_pkl, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "print(len(vocab))\n",
    "embedding = np.load(embd+'.npy')\n",
    "print(embedding.shape)\n",
    "glove_embd = embedding\n",
    "\n",
    "test_set = pd.read_csv('test_set_token.csv')\n",
    "train_set = pd.read_csv('train_set_token.csv')\n",
    "\n",
    "num_embeddings  = glove_embd.shape[0]\n",
    "embed_dim = glove_embd.shape[1]\n",
    "max_len = train_set.shape[1]-1\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Module, LSTM, ReLU, Linear, Sigmoid, Dropout, Embedding\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "glove_embd = torch.tensor(glove_embd)\n",
    "training = torch.tensor(train_set.values)\n",
    "\n",
    "net = Sequential(\n",
    "      Embedding(num_embeddings= num_embeddings, embedding_dim= embed_dim, _weight=glove_embd),\n",
    "      LSTM(input_size = embed_dim, hidden_size = 256, bidirectional = True),\n",
    "      LSTM(input_size = 256, hidden_size = 150, bidirectional = True),\n",
    "      Linear(150,128),\n",
    "      ReLU(),\n",
    "      Dropout(0.1),\n",
    "      Linear(128, 64),\n",
    "      ReLU(),\n",
    "      Linear(64,1),\n",
    "      Sigmoid()\n",
    "      )\n",
    "net.forward(training[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd2e759ece46f7968ecb77e124a003f65bcd7fd10d71dbb77ace05354c0e1a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
