{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_neg.txt', 'train_pos.txt', 'sample_submission.csv', 'test_data.txt', 'train_neg_clean.txt', 'train_pos_clean.txt', 'test_data_clean.txt']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from course_helpers import *\n",
    "from helpers import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Module, LSTM, ReLU, Linear, Sigmoid, Dropout, Embedding\n",
    "import torchvision\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "##data preprocessing \n",
    "DATA_PATH = 'twitter-datasets'\n",
    "Dataset = read(DATA_PATH)\n",
    "\n",
    "#@title read dataset file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset = clean(Dataset)\n",
    "# save_to_file(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash course_helpers.sh\n",
    "##Run this in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cut = 'vocab_cut.txt'\n",
    "vocab_pkl = 'vocab_pkl.pkl'\n",
    "coco_pkl = 'coco_pkl.pkl'\n",
    "embd = 'embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_vocab(vocab_cut, vocab_pkl)\n",
    "# cooc_(vocab_pkl, DATA_PATH, coco_pkl)\n",
    "# glove(coco_pkl, embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17971, 17971)\n",
      "17971\n",
      "(17971, 20)\n"
     ]
    }
   ],
   "source": [
    "with open(coco_pkl, \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "print(cooc.shape)\n",
    "with open(vocab_pkl, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "print(len(vocab))\n",
    "embedding = np.load(embd+'.npy')\n",
    "print(embedding.shape)\n",
    "glove_embd = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos, train_neg, test_set = read_data(Dataset)\n",
    "# train_set = pd.concat([train_pos, train_neg],  axis=0, ignore_index=True)\n",
    "# train_set = pd.concat([train_set.drop(labels='sentiment', axis=1), train_set['sentiment']], axis = 1)\n",
    "\n",
    "# train_set.loc[:, train_set.columns!='sentiment'] = train_set.applymap(lambda x: find_token(x, vocab))\n",
    "# test_set2 =  test_set.applymap(lambda x: find_token(x, vocab))\n",
    "# train_set.to_csv('train_set_token.csv', index= False)\n",
    "# test_set2.to_csv('test_set_token.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3029424/2353552957.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  glove_embd = torch.tensor(glove_embd)\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('test_set_token.csv')\n",
    "train_set = pd.read_csv('train_set_token.csv', )\n",
    "num_embeddings  = glove_embd.shape[0]\n",
    "embed_dim = glove_embd.shape[1]\n",
    "max_len = train_set.shape[1]-1\n",
    "batch_size = 16\n",
    "n_batches = train_set.shape[0]/batch_size\n",
    "\n",
    "X_train = train_set.loc[:, train_set.columns!='sentiment']\n",
    "Y_train = train_set['sentiment']\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "glove_embd = torch.tensor(glove_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = (X_train.shape[0]*0.2)%batch_size\n",
    "data_split  = X_train.shape[0]*0.2 - data_split\n",
    "data_split=data_split/2\n",
    "remd = -data_split + X_train.shape[0] - data_split\n",
    "addedv = remd%data_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val =  torch.tensor(pd.concat([X_train[:int(data_split)], X_train[-int(data_split):]],  axis=0, ignore_index=True).reset_index(drop=True).to_numpy()).to(device)\n",
    "Y_val =  torch.tensor(pd.concat([Y_train[:int(data_split)], Y_train[-int(data_split):]],  axis=0, ignore_index=True).reset_index(drop=True).to_numpy(dtype='float32')).to(device)\n",
    "X_train = torch.tensor(X_train[int(data_split+addedv):-int(data_split)].reset_index(drop=True).to_numpy()).to(device)\n",
    "Y_train =  torch.tensor(Y_train[int(data_split+addedv):-int(data_split)].reset_index(drop=True).to_numpy(dtype='float32')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =  torch.tensor(test_set.to_numpy()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_torch_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=[]):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.encodings[idx]\n",
    "        label = self.labels[idx]\n",
    "        return inp, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "class make_torch_testset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.encodings[idx]\n",
    "        return inp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "train_dataset = make_torch_dataset(X_train, Y_train)\n",
    "val_dataset = make_torch_dataset(X_val, Y_val)\n",
    "test_dataset = make_torch_testset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 143552 instances\n",
      "Validation set has 35888 instances\n",
      "Test set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(train_dataset)))\n",
    "print('Validation set has {} instances'.format(len(val_dataset)))\n",
    "print('Test set has {} instances'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class lstm_classifier(Module):\n",
    "    def __init__(self, num_embeddings, embed_dim, glove_embd, max_len) -> None:\n",
    "        super().__init__()\n",
    "        self.emd =  Embedding(num_embeddings= num_embeddings, embedding_dim= embed_dim, _weight=glove_embd)\n",
    "        self.lstm1 = LSTM(input_size = embed_dim, hidden_size = 256, bidirectional = True, batch_first  = True)\n",
    "        self.lstm2 = LSTM(input_size = 512, hidden_size = 150, bidirectional = True,  batch_first  = True)\n",
    "        self.fc1 = Linear(300,128)\n",
    "        self.drp = Dropout(0.1)\n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.fc3 = Linear(64,1)\n",
    "        self.reg = Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emd(x)\n",
    "        x = x.to(torch.float32)\n",
    "        x, states = self.lstm1(x)\n",
    "        x, states = self.lstm2(x)\n",
    "        x = F.relu(self.fc1(x[:,0]))\n",
    "        x = self.drp(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.reg(x)\n",
    "        x = x.reshape(x.shape[0])\n",
    "\n",
    "        return x\n",
    "model = lstm_classifier(num_embeddings, embed_dim, glove_embd, max_len).to(device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "\n",
    "    with tqdm(training_loader, unit=\"batch\") as tepoch:\n",
    "        for i, data in enumerate(tepoch):\n",
    "            tepoch.set_description(f\"Epoch {epoch_index}\")\n",
    "\n",
    "\n",
    "    #for i, data in enumerate(tqdm(training_loader), unit= \"batches\", ):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            accuracy  = accuracy_score(labels.cpu(), [1 if x > 0.5 else 0 for x in outputs.cpu()])\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += accuracy\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy = accuracy)\n",
    "            if i % batch_size == batch_size-1:\n",
    "                last_loss = running_loss / batch_size # loss per batch\n",
    "                #print('  epoch {} batch {} loss: {}'.format(epoch_index, (i + 1)/batch_size, last_loss))\n",
    "                tb_x = epoch_index * len(training_loader) + i + 1\n",
    "                #print(tb_x)\n",
    "                tb_writer.add_scalar('Loss/train', last_loss, global_step = tb_x)\n",
    "                running_loss = 0.\n",
    "\n",
    "    return last_loss, running_accuracy/n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/8972 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/8972 [00:00<1:51:06,  1.35batch/s, accuracy=0.562, loss=0.691]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n",
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/8972 [00:01<2:07:56,  1.17batch/s, accuracy=0.375, loss=0.702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 4/8972 [00:02<1:20:34,  1.86batch/s, accuracy=0.375, loss=0.698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "0.375\n",
      "1.875\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 4/8972 [00:02<1:20:34,  1.86batch/s, accuracy=0.25, loss=0.701] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.125\n",
      "0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 6/8972 [00:03<59:52,  2.50batch/s, accuracy=0.438, loss=0.694]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5625\n",
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 7/8972 [00:04<1:22:05,  1.82batch/s, accuracy=0.375, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9375\n",
      "0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 8/8972 [00:05<1:48:37,  1.38batch/s, accuracy=0.375, loss=0.694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3125\n",
      "0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 9/8972 [00:06<1:53:59,  1.31batch/s, accuracy=0.438, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 10/8972 [00:07<2:01:17,  1.23batch/s, accuracy=0.5, loss=0.693] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25\n",
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 11/8972 [00:07<2:06:36,  1.18batch/s, accuracy=0.562, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8125\n",
      "0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 11/8972 [00:08<1:56:25,  1.28batch/s, accuracy=0.562, loss=0.693]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m avg_loss, acc \u001b[39m=\u001b[39m train_one_epoch(epoch, writer)\n\u001b[1;32m     19\u001b[0m \u001b[39m# # We don't need gradients on to do reporting\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [17], line 29\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m---> 29\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     31\u001b[0m \u001b[39m# Adjust learning weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, acc = train_one_epoch(epoch, writer)\n",
    "\n",
    "    # # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs).detach()\n",
    "        vacc  = accuracy_score(vlabels.cpu(), [1 if x > 0.5 else 0 for x in voutputs.cpu()])\n",
    "        running_vacc += vacc\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vacc = running_vacc / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "    writer.add_scalars('Training vs. Validation Accuracy',\n",
    "                    { 'Training' : acc, 'Validation' : avg_vacc },\n",
    "                    epoch + 1)\n",
    "    #writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "preds_test = []\n",
    "for i, vdata in enumerate(test_loader):\n",
    "    vinputs = vdata\n",
    "    voutputs = model(vinputs).detach()\n",
    "    preds_test.extend(voutputs.cpu())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10000.0\n"
     ]
    }
   ],
   "source": [
    "preds = np.ones_like(preds_test)\n",
    "preds[np.where(np.array(preds_test)<0.5)] = -1\n",
    "preds = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1)+1, \"Prediction\": int(r2)})\n",
    "\n",
    "create_csv_submission(preds.index.values.tolist(), np.asarray(preds), \"____test____.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['train_neg.txt', 'train_pos.txt', 'sample_submission.csv', 'test_data.txt', 'train_neg_clean.txt', 'train_pos_clean.txt', 'test_data_clean.txt']\n",
      "(17971, 17971)\n",
      "17971\n",
      "(17971, 20)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from course_helpers import *\n",
    "from helpers import *\n",
    "\n",
    "\n",
    "\n",
    "##data preprocessing \n",
    "DATA_PATH = 'twitter-datasets'\n",
    "Dataset = read(DATA_PATH)\n",
    "\n",
    "#@title read dataset file\n",
    "vocab_cut = 'vocab_cut.txt'\n",
    "vocab_pkl = 'vocab_pkl.pkl'\n",
    "coco_pkl = 'coco_pkl.pkl'\n",
    "embd = 'embeddings'\n",
    "with open(coco_pkl, \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "print(cooc.shape)\n",
    "with open(vocab_pkl, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "print(len(vocab))\n",
    "embedding = np.load(embd+'.npy')\n",
    "print(embedding.shape)\n",
    "glove_embd = embedding\n",
    "\n",
    "test_set = pd.read_csv('test_set_token.csv')\n",
    "train_set = pd.read_csv('train_set_token.csv')\n",
    "\n",
    "num_embeddings  = glove_embd.shape[0]\n",
    "embed_dim = glove_embd.shape[1]\n",
    "max_len = train_set.shape[1]-1\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Module, LSTM, ReLU, Linear, Sigmoid, Dropout, Embedding\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "glove_embd = torch.tensor(glove_embd)\n",
    "training = torch.tensor(train_set.values)\n",
    "\n",
    "net = Sequential(\n",
    "      Embedding(num_embeddings= num_embeddings, embedding_dim= embed_dim, _weight=glove_embd),\n",
    "      LSTM(input_size = embed_dim, hidden_size = 256, bidirectional = True),\n",
    "      LSTM(input_size = 256, hidden_size = 150, bidirectional = True),\n",
    "      Linear(150,128),\n",
    "      ReLU(),\n",
    "      Dropout(0.1),\n",
    "      Linear(128, 64),\n",
    "      ReLU(),\n",
    "      Linear(64,1),\n",
    "      Sigmoid()\n",
    "      )\n",
    "net.forward(training[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd2e759ece46f7968ecb77e124a003f65bcd7fd10d71dbb77ace05354c0e1a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
