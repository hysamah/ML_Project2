{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from course_helpers import *\n",
    "from helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_vocab(vocab_cut, vocab_pkl)\n",
    "#cooc_(vocab_pkl, DATA_PATH, coco_pkl)\n",
    "#glove(coco_pkl, embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21161, 21161)\n",
      "21161\n",
      "(21161, 20)\n"
     ]
    }
   ],
   "source": [
    "with open(coco_pkl, \"rb\") as f:\n",
    "        cooc = pickle.load(f)\n",
    "print(cooc.shape)\n",
    "with open(vocab_pkl, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "print(len(vocab))\n",
    "embedding = np.load(embd+'.npy')\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, train_neg, test_set = read_data(DATA_PATH)\n",
    "train_set = pd.concat([train_pos, train_neg],  axis=0)\n",
    "glove_embd = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysam\\AppData\\Local\\Temp\\ipykernel_17652\\3939322011.py:1: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  train_set.loc[:, train_set.columns!='sentiment'] = train_set.applymap(lambda x: find_token(x, vocab))\n"
     ]
    }
   ],
   "source": [
    "train_set.loc[:, train_set.columns!='sentiment'] = train_set.applymap(lambda x: find_token(x, vocab))\n",
    "test_set2 =  test_set.applymap(lambda x: find_token(x, vocab))\n",
    "#train_set.loc[:, train_set.columns!='sentiment'] = train_set.loc[:, train_set.columns!='sentiment'].applymap(lambda x: findembedding(x, glove_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning: \n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Embedding, concatenate, Dropout, concatenate, Input\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "class RnnModel():\n",
    "    \"\"\"\n",
    "    A recurrent neural network for semantic analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix, embedding_dim, max_len, X_additional=None):\n",
    "        \n",
    "        inp1 = Input(shape=(max_len,))\n",
    "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "        x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(150))(x)\n",
    "        x = Dense(128, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)    \n",
    "        model = Model(inputs=inp1, outputs=x)\n",
    "\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "embed_dim = glove_embd.shape[1]\n",
    "max_len = train_set.shape[1]-1\n",
    "X_train = train_set.loc[:, train_set.columns!='sentiment']\n",
    "Y_train = train_set['sentiment']\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysam\\AppData\\Local\\Temp\\ipykernel_17652\\1328371404.py:3: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  Y_subset =  [ Y_train[:2000],  Y_train[-2000:]]\n"
     ]
    }
   ],
   "source": [
    "X_subset =  [ X_train[:2000],  X_train[-2000:]]\n",
    "X_subset = pd.concat(X_subset)\n",
    "Y_subset =  [ Y_train[:2000],  Y_train[-2000:]]\n",
    "Y_subset = pd.concat(Y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12500/12500 [==============================] - 3342s 267ms/step - loss: 0.4563\n",
      "Epoch 2/5\n",
      "12500/12500 [==============================] - 6997s 560ms/step - loss: 0.3581\n",
      "Epoch 3/5\n",
      "12500/12500 [==============================] - 4668s 373ms/step - loss: 0.3304\n",
      "Epoch 4/5\n",
      "12500/12500 [==============================] - 4018s 321ms/step - loss: 0.3089\n",
      "Epoch 5/5\n",
      "12500/12500 [==============================] - 5483s 439ms/step - loss: 0.2844\n"
     ]
    }
   ],
   "source": [
    "rnn = RnnModel(\n",
    "    embedding_matrix=glove_embd, \n",
    "    embedding_dim=embed_dim, \n",
    "    max_len=max_len\n",
    ")\n",
    "rnn.model.fit(\n",
    "    X_train,\n",
    "    Y_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "model = rnn.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 58ms/step\n",
      "13/13 [==============================] - 1s 51ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysam\\AppData\\Local\\Temp\\ipykernel_17652\\1386457197.py:4: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  gndt = np.append(Y_train[-2500:-2100], Y_train[2100:2500]).tolist()\n"
     ]
    }
   ],
   "source": [
    "preds1 = rnn.model.predict(X_train[-2500:-2100])\n",
    "preds2=rnn.model.predict(X_train[2100:2500])\n",
    "preds = np.append(preds1, preds2).tolist()\n",
    "gndt = np.append(Y_train[-2500:-2100], Y_train[2100:2500]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89625"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(gndt, [1 if x > 0.5 else 0 for x in preds])\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#predicting on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 13s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_test = rnn.model.predict(test_set2)\n",
    "preds = np.ones_like(preds_test)\n",
    "preds[np.where(preds_test<0.5)] = -1\n",
    "preds = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0    -1.0\n",
       "1    -1.0\n",
       "2    -1.0\n",
       "3    -1.0\n",
       "4    -1.0\n",
       "...   ...\n",
       "9995  1.0\n",
       "9996 -1.0\n",
       "9997 -1.0\n",
       "9998  1.0\n",
       "9999 -1.0\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1)+1, \"Prediction\": int(r2)})\n",
    "\n",
    "create_csv_submission(preds.index.values.tolist(), np.asarray(preds), \"submission1_basic_lstm_bad_token.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Plotting for tokenization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the number of occurence of different words\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    DESCRIPTION: \n",
    "            Reads a file and returns it as a list\n",
    "    INPUT: \n",
    "            filename: Name of the file to be read\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename, \"r\", encoding='utf8') as ins:\n",
    "        for line in ins:\n",
    "            #line = line.split(' ')\n",
    "            data.append(line.strip())\n",
    "    return data\n",
    "full_vocab = pd.DataFrame(read_file('vocab_full.txt'))\n",
    "full_vocab = full_vocab[0].str.split(' ', expand=True)\n",
    "full_vocab = full_vocab.astype({0:'int'})\n",
    "full_vocab.sort_values(by=[0], ascending=False, inplace= True)\n",
    "x = full_vocab[1]\n",
    "y = full_vocab[0]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd2e759ece46f7968ecb77e124a003f65bcd7fd10d71dbb77ace05354c0e1a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
